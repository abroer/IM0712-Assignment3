{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e54348c56bccb4",
   "metadata": {},
   "source": [
    "The article \"prediction of Wine Quality Using Machine Learning\" states that it used the red wine dataset. Paragraph 2.1 mentions that the dataset contains 4898 entries. When examing the datasets, this number corresponds to the white wine dataset. This confusion in consistent throughout the article. From this point on the white wine dataset will be used, eventhough the article states that the red wine dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "id": "589ed18d32af467f",
   "metadata": {},
   "source": [
    "# used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from shap import initjs\n",
    "\n",
    "print(tf.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n",
    "from keras.optimizers import Adam"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2b2c56e79607620",
   "metadata": {},
   "source": [
    "#import the winequality dataset\n",
    "\n",
    "# Define the relative path to the CSV file\n",
    "relative_path = 'wine+quality/winequality-white.csv'\n",
    "# Read the CSV file using pandas\n",
    "wine_df = pd.read_csv(relative_path, delimiter=';')\n",
    "\n",
    "features = wine_df.drop('quality', axis=1)\n",
    "target = wine_df['quality']\n",
    "\n",
    "len(features)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce503e930553211e",
   "metadata": {},
   "source": [
    "Feature scaling was applied using standardization. Following code standardizes the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "772b0e88e0800a59",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "feature_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "feature_scaled_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c040c434f4e3623",
   "metadata": {},
   "source": [
    "Create the train, validation and test set from the original data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "952a9fe83aaf8d81",
   "metadata": {},
   "source": [
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(features_scaled, target, test_size=0.4, random_state=1)\n",
    "\n",
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "features_test, features_validate, target_test, target_validate = train_test_split(features_temp, target_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "# Now, features_train and target_train contain the training data (60% of the original data)\n",
    "# features_test and target_test contain the testing data (20% of the original data)\n",
    "# features_validate and target_validate contain the validation data (20% of the original data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "726c2693f7ab3f4a",
   "metadata": {},
   "source": [
    "Create a ANN model with:\n",
    "- 1 input layer with 11 neurons\n",
    "- 3 hidden layers with 15 neurons\n",
    "- 1 output layer with 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8a03c385920b3de",
   "metadata": {},
   "source": [
    "# Initialize the Sequential model\n",
    "model = tf.keras.Sequential()\n",
    "# model = Sequential()\n",
    "\n",
    "#add layers\n",
    "model.add(Input(shape=(11,))) # input layer\n",
    "model.add(Dense(15, activation='relu')) # first hidden layer\n",
    "model.add(Dense(15, activation='relu')) # second hidden layer\n",
    "model.add(Dense(15, activation='relu')) # third hidden layer\n",
    "model.add(Dense(1, activation='linear')) # Output layer\n",
    "\n",
    "# Compile the model with mean squared error loss and Adam optimizer\n",
    "# Also include mean squared error, mean absolute error, and mean absolute percentage error as metrics\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "846124d0e3968faa",
   "metadata": {},
   "source": [
    "# Train the model with the training data\n",
    "# - epochs: Number of times the model will iterate over the entire training dataset\n",
    "# - batch_size: Number of samples per gradient update\n",
    "# - validation_data: Data on which to evaluate the loss and metrics at the end of each epoch\n",
    "# - verbose: Verbosity mode (1 = progress bar)\n",
    "history = model.fit(features_train, target_train, epochs=50, batch_size=32, validation_data=(features_validate, target_validate), verbose=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fab0a3ba9ab68eeb",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(features_test, target_test, verbose=1)\n",
    "\n",
    "# Print the test loss\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(features_test)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fae597cb",
   "metadata": {},
   "source": [
    "# Function to calculate metrics (R, MSE, MAPE)\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Calculate R (Pearson correlation coefficient)\n",
    "    r, _ = pearsonr(y_true, y_pred)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    return r, mse, mape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68c90599",
   "metadata": {},
   "source": [
    "# Make predictions on the training and test data\n",
    "train_predictions = model.predict(features_train)\n",
    "test_predictions = model.predict(features_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7b2246a",
   "metadata": {},
   "source": [
    "# Convert to 1D numpy array\n",
    "train_predictions = train_predictions.flatten()\n",
    "test_predictions = test_predictions.flatten()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "060e2edc",
   "metadata": {},
   "source": [
    "# Calculate metrics for training data set\n",
    "train_r, train_mse, train_mape = calculate_metrics(target_train, train_predictions)\n",
    "\n",
    "# # Calculate metrics for testing data set\n",
    "test_r, test_mse, test_mape = calculate_metrics(target_test, test_predictions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b47d038adc0074f",
   "metadata": {},
   "source": [
    "import shap\n",
    "\n",
    "background_data = features_train  # Use a subset of the training data for background\n",
    "test_data = features_test[:5]\n",
    "\n",
    "explainer = shap.KernelExplainer(model, background_data)\n",
    "shap_values = explainer.shap_values(test_data)\n",
    "fig, ax = plt.subplots(4, 3, figsize=(20, 15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(11):\n",
    "    shap.dependence_plot(i, shap_values, test_data, feature_names=features.columns, ax=ax[i], show=False)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(11, len(ax)):\n",
    "    fig.delaxes(ax[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#shap.dependence_plot(2, shap_values, test_data, feature_names=features.columns)\n",
    "#shap.summary_plot(shap_values, test_data, feature_names=features.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8aec7e0b543c8add",
   "metadata": {},
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(explainer.expected_value[0], shap_values[..., 0], feature_names=features.columns)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
