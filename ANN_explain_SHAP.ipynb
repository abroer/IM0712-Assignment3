{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e54348c56bccb4",
   "metadata": {},
   "source": [
    "The article \"prediction of Wine Quality Using Machine Learning\" states that it used the red wine dataset. Paragraph 2.1 mentions that the dataset contains 4898 entries. When examing the datasets, this number corresponds to the white wine dataset. This confusion in consistent throughout the article. From this point on the white wine dataset will be used, eventhough the article states that the red wine dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "id": "589ed18d32af467f",
   "metadata": {},
   "source": [
    "from tabnanny import verbose\n",
    "\n",
    "# used libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from shap import initjs\n",
    "\n",
    "print(tf.__version__)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n",
    "from keras.optimizers import Adam"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2b2c56e79607620",
   "metadata": {},
   "source": [
    "#import the winequality dataset\n",
    "\n",
    "# Define the relative path to the CSV file\n",
    "relative_path = 'wine+quality/winequality-white.csv'\n",
    "# Read the CSV file using pandas\n",
    "wine_df = pd.read_csv(relative_path, delimiter=';')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wine_scaled_df = pd.DataFrame(scaler.fit_transform(wine_df), columns=wine_df.columns)\n",
    "\n",
    "X = wine_scaled_df.drop('quality', axis=1)\n",
    "y = wine_df['quality']\n",
    "\n",
    "len(X)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ce503e930553211e",
   "metadata": {},
   "source": [
    "Feature scaling was applied using standardization. Following code standardizes the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040c434f4e3623",
   "metadata": {},
   "source": [
    "Create the train, validation and test set from the original data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "952a9fe83aaf8d81",
   "metadata": {},
   "source": [
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "print(f'Original set and target shape: {X.shape} {y.shape}')\n",
    "print(f'Training set and target shape: {X_train.shape} {y_train.shape}')\n",
    "print(f'Validation set and target shape: {X_validate.shape} {y_validate.shape}')\n",
    "print(f'Test set and target shape: {X_test.shape} {y_test.shape}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "726c2693f7ab3f4a",
   "metadata": {},
   "source": [
    "Create a ANN model with:\n",
    "- 1 input layer with 11 neurons\n",
    "- 3 hidden layers with 15 neurons\n",
    "- 1 output layer with 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8a03c385920b3de",
   "metadata": {},
   "source": [
    "# Initialize the Sequential model\n",
    "model = tf.keras.Sequential()\n",
    "# model = Sequential()\n",
    "\n",
    "#add layers\n",
    "model.add(Input(shape=(11,))) # input layer\n",
    "model.add(Dense(15, activation='relu')) # first hidden layer\n",
    "model.add(Dense(15, activation='relu')) # second hidden layer\n",
    "model.add(Dense(15, activation='relu')) # third hidden layer\n",
    "model.add(Dense(1, activation='linear')) # Output layer\n",
    "\n",
    "# Compile the model with mean squared error loss and Adam optimizer\n",
    "# Also include mean squared error, mean absolute error, and mean absolute percentage error as metrics\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "846124d0e3968faa",
   "metadata": {},
   "source": [
    "# Train the model with the training data\n",
    "# - epochs: Number of times the model will iterate over the entire training dataset\n",
    "# - batch_size: Number of samples per gradient update\n",
    "# - validation_data: Data on which to evaluate the loss and metrics at the end of each epoch\n",
    "# - verbose: Verbosity mode (1 = progress bar)\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_validate, y_validate), verbose=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fab0a3ba9ab68eeb",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test loss\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_train_pred = model.predict(X_train, verbose=0)\n",
    "y_test_pred = model.predict(X_test, verbose=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fae597cb",
   "metadata": {},
   "source": [
    "# Function to calculate metrics (R, MSE, MAPE)\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Calculate R (Pearson correlation coefficient)\n",
    "    r, _ = pearsonr(y_true, y_pred)\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    return r, mse, mape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68c90599",
   "metadata": {},
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y, y_pred, s=10, alpha=0.1, label='All Data')\n",
    "plt.plot([0,9],[0,9],color='red', linestyle='-', label='Perfect Prediction')\n",
    "plt.xlabel('Target Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "060e2edc",
   "metadata": {},
   "source": [
    "# Calculate metrics for training data set\n",
    "train_r, train_mse, train_mape = calculate_metrics(y_train, train_predictions)\n",
    "\n",
    "# # Calculate metrics for testing data set\n",
    "test_r, test_mse, test_mape = calculate_metrics(y_test, test_predictions)\n",
    "\n",
    "print(f\"Training Data Metrics: R: {train_r:.4f}, MSE: {train_mse:.4f}, MAPE: {train_mape:.4f}\")\n",
    "print(f\"Testing Data Metrics: R: {test_r:.4f}, MSE: {test_mse:.4f}, MAPE: {test_mape:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b47d038adc0074f",
   "metadata": {},
   "source": [
    "import shap\n",
    "\n",
    "background_data = X_train.sample(100, random_state=1)\n",
    "test_data = X_test.sample(11, random_state=1)\n",
    "\n",
    "explainer = shap.KernelExplainer(model.predict, background_data)\n",
    "shap_values = explainer.shap_values(test_data)\n",
    "\n",
    "shap.summary_plot(shap_values, test_data, feature_names=X.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# print the dependence plot for all the features\n",
    "fig, ax = plt.subplots(5, 3, figsize=(20, 15))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(11):\n",
    "    shap.dependence_plot(i, shap_values, test_data, feature_names=X.columns, ax=ax[i], show=False)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(11, len(ax)):\n",
    "    fig.delaxes(ax[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "5cfbdf4e950db0bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8aec7e0b543c8add",
   "metadata": {},
   "source": [
    "shap.initjs()\n",
    "shap.plots.force(explainer.expected_value[0], shap_values[..., 0], feature_names=X.columns)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
