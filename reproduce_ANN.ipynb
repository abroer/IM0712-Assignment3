{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e54348c56bccb4",
   "metadata": {},
   "source": [
    "The article \"prediction of Wine Quality Using Machine Learning\" states that it used the red wine dataset. Paragraph 2.1 mentions that the dataset contains 4898 entries. When examing the datasets, this number corresponds to the white wine dataset. This confusion in consistent throughout the article. From this point on the white wine dataset will be used, eventhough the article states that the red wine dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "id": "589ed18d32af467f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:57:28.186651Z",
     "start_time": "2025-04-05T20:57:27.718195Z"
    }
   },
   "source": [
    "from xml.sax.handler import feature_string_interning\n",
    "\n",
    "# used libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "f2b2c56e79607620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:10.522377Z",
     "start_time": "2025-04-05T20:52:10.491874Z"
    }
   },
   "source": [
    "#import the winequality dataset\n",
    "\n",
    "# Define the relative path to the CSV file\n",
    "relative_path = 'wine+quality/winequality-white.csv'\n",
    "# Read the CSV file using pandas\n",
    "wine_df = pd.read_csv(relative_path, delimiter=';')\n",
    "\n",
    "features = wine_df.drop('quality', axis=1)\n",
    "target = wine_df['quality']\n",
    "\n",
    "len(features)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ce503e930553211e",
   "metadata": {},
   "source": [
    "Feature scaling was applied using standardization. Following code standardizes the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "772b0e88e0800a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:10.745238Z",
     "start_time": "2025-04-05T20:52:10.700733Z"
    }
   },
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "feature_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "feature_scaled_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0          0.172097         -0.081770     0.213280        2.821349  -0.035355   \n",
       "1         -0.657501          0.215896     0.048001       -0.944765   0.147747   \n",
       "2          1.475751          0.017452     0.543838        0.100282   0.193523   \n",
       "3          0.409125         -0.478657    -0.117278        0.415768   0.559727   \n",
       "4          0.409125         -0.478657    -0.117278        0.415768   0.559727   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893      -0.776015         -0.677101    -0.365197       -0.944765  -0.310008   \n",
       "4894      -0.301959          0.414339     0.213280        0.317179   0.056196   \n",
       "4895      -0.420473         -0.379435    -1.191592       -1.023637  -0.218457   \n",
       "4896      -1.605613          0.116674    -0.282557       -1.043355  -1.088192   \n",
       "4897      -1.013043         -0.677101     0.378559       -1.102508  -1.179743   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0                0.569932              0.744565  2.331512 -1.246921   \n",
       "1               -1.253019             -0.149685 -0.009154  0.740029   \n",
       "2               -0.312141             -0.973336  0.358665  0.475102   \n",
       "3                0.687541              1.121091  0.525855  0.011480   \n",
       "4                0.687541              1.121091  0.525855  0.011480   \n",
       "...                   ...                   ...       ...       ...   \n",
       "4893            -0.664970             -1.091000 -0.965483  0.541334   \n",
       "4894             1.275590              0.697499  0.291789 -0.253446   \n",
       "4895            -0.312141             -0.643875 -0.497350 -1.313153   \n",
       "4896            -0.900190             -0.667408 -1.784717  1.004955   \n",
       "4897            -0.782580             -0.949803 -1.543962  0.475102   \n",
       "\n",
       "      sulphates   alcohol  \n",
       "0     -0.349184 -1.393152  \n",
       "1      0.001342 -0.824276  \n",
       "2     -0.436816 -0.336667  \n",
       "3     -0.787342 -0.499203  \n",
       "4     -0.787342 -0.499203  \n",
       "...         ...       ...  \n",
       "4893   0.088973  0.557282  \n",
       "4894  -0.261553 -0.743008  \n",
       "4895  -0.261553 -0.905544  \n",
       "4896  -0.962605  1.857572  \n",
       "4897  -1.488394  1.044891  \n",
       "\n",
       "[4898 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172097</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>2.821349</td>\n",
       "      <td>-0.035355</td>\n",
       "      <td>0.569932</td>\n",
       "      <td>0.744565</td>\n",
       "      <td>2.331512</td>\n",
       "      <td>-1.246921</td>\n",
       "      <td>-0.349184</td>\n",
       "      <td>-1.393152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.657501</td>\n",
       "      <td>0.215896</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>0.147747</td>\n",
       "      <td>-1.253019</td>\n",
       "      <td>-0.149685</td>\n",
       "      <td>-0.009154</td>\n",
       "      <td>0.740029</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>-0.824276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.475751</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.543838</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>0.193523</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.973336</td>\n",
       "      <td>0.358665</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-0.436816</td>\n",
       "      <td>-0.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409125</td>\n",
       "      <td>-0.478657</td>\n",
       "      <td>-0.117278</td>\n",
       "      <td>0.415768</td>\n",
       "      <td>0.559727</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>1.121091</td>\n",
       "      <td>0.525855</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.787342</td>\n",
       "      <td>-0.499203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>-0.776015</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>-0.365197</td>\n",
       "      <td>-0.944765</td>\n",
       "      <td>-0.310008</td>\n",
       "      <td>-0.664970</td>\n",
       "      <td>-1.091000</td>\n",
       "      <td>-0.965483</td>\n",
       "      <td>0.541334</td>\n",
       "      <td>0.088973</td>\n",
       "      <td>0.557282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>-0.301959</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.213280</td>\n",
       "      <td>0.317179</td>\n",
       "      <td>0.056196</td>\n",
       "      <td>1.275590</td>\n",
       "      <td>0.697499</td>\n",
       "      <td>0.291789</td>\n",
       "      <td>-0.253446</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.743008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>-0.420473</td>\n",
       "      <td>-0.379435</td>\n",
       "      <td>-1.191592</td>\n",
       "      <td>-1.023637</td>\n",
       "      <td>-0.218457</td>\n",
       "      <td>-0.312141</td>\n",
       "      <td>-0.643875</td>\n",
       "      <td>-0.497350</td>\n",
       "      <td>-1.313153</td>\n",
       "      <td>-0.261553</td>\n",
       "      <td>-0.905544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>-1.605613</td>\n",
       "      <td>0.116674</td>\n",
       "      <td>-0.282557</td>\n",
       "      <td>-1.043355</td>\n",
       "      <td>-1.088192</td>\n",
       "      <td>-0.900190</td>\n",
       "      <td>-0.667408</td>\n",
       "      <td>-1.784717</td>\n",
       "      <td>1.004955</td>\n",
       "      <td>-0.962605</td>\n",
       "      <td>1.857572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>-1.013043</td>\n",
       "      <td>-0.677101</td>\n",
       "      <td>0.378559</td>\n",
       "      <td>-1.102508</td>\n",
       "      <td>-1.179743</td>\n",
       "      <td>-0.782580</td>\n",
       "      <td>-0.949803</td>\n",
       "      <td>-1.543962</td>\n",
       "      <td>0.475102</td>\n",
       "      <td>-1.488394</td>\n",
       "      <td>1.044891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c040c434f4e3623",
   "metadata": {},
   "source": [
    "Create the train, validation and test set from the original data set"
   ]
  },
  {
   "cell_type": "code",
   "id": "952a9fe83aaf8d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:10.876958Z",
     "start_time": "2025-04-05T20:52:10.869217Z"
    }
   },
   "source": [
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(features_scaled, target, test_size=0.4, random_state=1)\n",
    "\n",
    "# Step 1: Split the data into training and temporary sets\n",
    "# The training set will contain 60% of the data, and the temporary set will contain 40% of the data\n",
    "features_test, features_validate, target_test, target_validate = train_test_split(features_temp, target_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "# Now, features_train and target_train contain the training data (60% of the original data)\n",
    "# features_test and target_test contain the testing data (20% of the original data)\n",
    "# features_validate and target_validate contain the validation data (20% of the original data)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "726c2693f7ab3f4a",
   "metadata": {},
   "source": [
    "Create a ANN model with:\n",
    "- 1 input layer with 11 neurons\n",
    "- 3 hidden layers with 15 neurons\n",
    "- 1 output layer with 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8a03c385920b3de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:11.142960Z",
     "start_time": "2025-04-05T20:52:10.967855Z"
    }
   },
   "source": [
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers\n",
    "model.add(Input(shape=(11,))) # input layer\n",
    "model.add(Dense(15, activation='relu')) # first hidden layer\n",
    "model.add(Dense(15, activation='relu')) # second hidden layer\n",
    "model.add(Dense(15, activation='relu')) # third hidden layer\n",
    "model.add(Dense(1, activation='linear')) # Output layer\n",
    "\n",
    "# Compile the model with mean squared error loss and Adam optimizer\n",
    "# Also include mean squared error, mean absolute error, and mean absolute percentage error as metrics\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "\n",
    "# Print the summary of the model architecture\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 22:52:10.981722: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m)             │           \u001B[38;5;34m180\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m)             │           \u001B[38;5;34m240\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m)             │           \u001B[38;5;34m240\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │            \u001B[38;5;34m16\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m676\u001B[0m (2.64 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">676</span> (2.64 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m676\u001B[0m (2.64 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">676</span> (2.64 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "846124d0e3968faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:22.600566Z",
     "start_time": "2025-04-05T20:52:11.230707Z"
    }
   },
   "source": [
    "# Train the model with the training data\n",
    "# - epochs: Number of times the model will iterate over the entire training dataset\n",
    "# - batch_size: Number of samples per gradient update\n",
    "# - validation_data: Data on which to evaluate the loss and metrics at the end of each epoch\n",
    "# - verbose: Verbosity mode (1 = progress bar)\n",
    "history = model.fit(features_train, target_train, epochs=50, batch_size=32, validation_data=(features_validate, target_validate), verbose=1)\n",
    "\n",
    "# Print the training history\n",
    "# history.history contains the loss and metrics values for each epoch\n",
    "print(history.history)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 3ms/step - loss: 27.8201 - mean_absolute_error: 5.1075 - mean_absolute_percentage_error: 86.8966 - mean_squared_error: 27.8201 - val_loss: 6.8277 - val_mean_absolute_error: 2.0868 - val_mean_absolute_percentage_error: 35.7084 - val_mean_squared_error: 6.8277\n",
      "Epoch 2/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 5.1427 - mean_absolute_error: 1.7556 - mean_absolute_percentage_error: 30.6294 - mean_squared_error: 5.1427 - val_loss: 3.1267 - val_mean_absolute_error: 1.3532 - val_mean_absolute_percentage_error: 23.4582 - val_mean_squared_error: 3.1267\n",
      "Epoch 3/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 2.6203 - mean_absolute_error: 1.2300 - mean_absolute_percentage_error: 21.2304 - mean_squared_error: 2.6203 - val_loss: 2.1788 - val_mean_absolute_error: 1.1387 - val_mean_absolute_percentage_error: 19.7340 - val_mean_squared_error: 2.1788\n",
      "Epoch 4/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - loss: 1.9066 - mean_absolute_error: 1.0702 - mean_absolute_percentage_error: 18.7199 - mean_squared_error: 1.9066 - val_loss: 1.7323 - val_mean_absolute_error: 1.0207 - val_mean_absolute_percentage_error: 17.7229 - val_mean_squared_error: 1.7323\n",
      "Epoch 5/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1.6583 - mean_absolute_error: 0.9725 - mean_absolute_percentage_error: 17.2421 - mean_squared_error: 1.6583 - val_loss: 1.4672 - val_mean_absolute_error: 0.9387 - val_mean_absolute_percentage_error: 16.3828 - val_mean_squared_error: 1.4672\n",
      "Epoch 6/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1.3822 - mean_absolute_error: 0.9074 - mean_absolute_percentage_error: 15.8107 - mean_squared_error: 1.3822 - val_loss: 1.2491 - val_mean_absolute_error: 0.8671 - val_mean_absolute_percentage_error: 15.0297 - val_mean_squared_error: 1.2491\n",
      "Epoch 7/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - loss: 1.1679 - mean_absolute_error: 0.8249 - mean_absolute_percentage_error: 14.4537 - mean_squared_error: 1.1679 - val_loss: 1.0849 - val_mean_absolute_error: 0.8075 - val_mean_absolute_percentage_error: 14.1070 - val_mean_squared_error: 1.0849\n",
      "Epoch 8/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 1.0695 - mean_absolute_error: 0.7891 - mean_absolute_percentage_error: 13.8714 - mean_squared_error: 1.0695 - val_loss: 0.9646 - val_mean_absolute_error: 0.7610 - val_mean_absolute_percentage_error: 13.2356 - val_mean_squared_error: 0.9646\n",
      "Epoch 9/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.8824 - mean_absolute_error: 0.7210 - mean_absolute_percentage_error: 12.7293 - mean_squared_error: 0.8824 - val_loss: 0.8592 - val_mean_absolute_error: 0.7170 - val_mean_absolute_percentage_error: 12.4930 - val_mean_squared_error: 0.8592\n",
      "Epoch 10/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.8344 - mean_absolute_error: 0.6983 - mean_absolute_percentage_error: 12.2385 - mean_squared_error: 0.8344 - val_loss: 0.7916 - val_mean_absolute_error: 0.6912 - val_mean_absolute_percentage_error: 11.9230 - val_mean_squared_error: 0.7916\n",
      "Epoch 11/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.7716 - mean_absolute_error: 0.6777 - mean_absolute_percentage_error: 11.9138 - mean_squared_error: 0.7716 - val_loss: 0.7351 - val_mean_absolute_error: 0.6668 - val_mean_absolute_percentage_error: 11.7347 - val_mean_squared_error: 0.7351\n",
      "Epoch 12/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.7056 - mean_absolute_error: 0.6505 - mean_absolute_percentage_error: 11.5217 - mean_squared_error: 0.7056 - val_loss: 0.6838 - val_mean_absolute_error: 0.6439 - val_mean_absolute_percentage_error: 11.2732 - val_mean_squared_error: 0.6838\n",
      "Epoch 13/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.6482 - mean_absolute_error: 0.6247 - mean_absolute_percentage_error: 10.8642 - mean_squared_error: 0.6482 - val_loss: 0.6604 - val_mean_absolute_error: 0.6341 - val_mean_absolute_percentage_error: 11.2042 - val_mean_squared_error: 0.6604\n",
      "Epoch 14/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.6550 - mean_absolute_error: 0.6249 - mean_absolute_percentage_error: 11.0919 - mean_squared_error: 0.6550 - val_loss: 0.6308 - val_mean_absolute_error: 0.6193 - val_mean_absolute_percentage_error: 10.9185 - val_mean_squared_error: 0.6308\n",
      "Epoch 15/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.6149 - mean_absolute_error: 0.6126 - mean_absolute_percentage_error: 10.8298 - mean_squared_error: 0.6149 - val_loss: 0.5964 - val_mean_absolute_error: 0.6022 - val_mean_absolute_percentage_error: 10.5844 - val_mean_squared_error: 0.5964\n",
      "Epoch 16/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.6026 - mean_absolute_error: 0.6037 - mean_absolute_percentage_error: 10.6516 - mean_squared_error: 0.6026 - val_loss: 0.5772 - val_mean_absolute_error: 0.5924 - val_mean_absolute_percentage_error: 10.3498 - val_mean_squared_error: 0.5772\n",
      "Epoch 17/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5730 - mean_absolute_error: 0.5952 - mean_absolute_percentage_error: 10.5413 - mean_squared_error: 0.5730 - val_loss: 0.5653 - val_mean_absolute_error: 0.5860 - val_mean_absolute_percentage_error: 10.3110 - val_mean_squared_error: 0.5653\n",
      "Epoch 18/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5820 - mean_absolute_error: 0.5899 - mean_absolute_percentage_error: 10.4730 - mean_squared_error: 0.5820 - val_loss: 0.5837 - val_mean_absolute_error: 0.5962 - val_mean_absolute_percentage_error: 10.6562 - val_mean_squared_error: 0.5837\n",
      "Epoch 19/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5848 - mean_absolute_error: 0.5971 - mean_absolute_percentage_error: 10.5635 - mean_squared_error: 0.5848 - val_loss: 0.5454 - val_mean_absolute_error: 0.5762 - val_mean_absolute_percentage_error: 10.1054 - val_mean_squared_error: 0.5454\n",
      "Epoch 20/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5261 - mean_absolute_error: 0.5654 - mean_absolute_percentage_error: 9.9673 - mean_squared_error: 0.5261 - val_loss: 0.5395 - val_mean_absolute_error: 0.5709 - val_mean_absolute_percentage_error: 9.9762 - val_mean_squared_error: 0.5395\n",
      "Epoch 21/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step - loss: 0.5418 - mean_absolute_error: 0.5792 - mean_absolute_percentage_error: 10.2244 - mean_squared_error: 0.5418 - val_loss: 0.5411 - val_mean_absolute_error: 0.5730 - val_mean_absolute_percentage_error: 10.1821 - val_mean_squared_error: 0.5411\n",
      "Epoch 22/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5336 - mean_absolute_error: 0.5688 - mean_absolute_percentage_error: 10.1009 - mean_squared_error: 0.5336 - val_loss: 0.5406 - val_mean_absolute_error: 0.5751 - val_mean_absolute_percentage_error: 10.2829 - val_mean_squared_error: 0.5406\n",
      "Epoch 23/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5176 - mean_absolute_error: 0.5652 - mean_absolute_percentage_error: 9.9869 - mean_squared_error: 0.5176 - val_loss: 0.5341 - val_mean_absolute_error: 0.5719 - val_mean_absolute_percentage_error: 10.2021 - val_mean_squared_error: 0.5341\n",
      "Epoch 24/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5478 - mean_absolute_error: 0.5778 - mean_absolute_percentage_error: 10.1912 - mean_squared_error: 0.5478 - val_loss: 0.5143 - val_mean_absolute_error: 0.5605 - val_mean_absolute_percentage_error: 9.7678 - val_mean_squared_error: 0.5143\n",
      "Epoch 25/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5719 - mean_absolute_error: 0.5761 - mean_absolute_percentage_error: 10.3899 - mean_squared_error: 0.5719 - val_loss: 0.5107 - val_mean_absolute_error: 0.5594 - val_mean_absolute_percentage_error: 9.7084 - val_mean_squared_error: 0.5107\n",
      "Epoch 26/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5302 - mean_absolute_error: 0.5718 - mean_absolute_percentage_error: 10.0742 - mean_squared_error: 0.5302 - val_loss: 0.5076 - val_mean_absolute_error: 0.5580 - val_mean_absolute_percentage_error: 9.6470 - val_mean_squared_error: 0.5076\n",
      "Epoch 27/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4931 - mean_absolute_error: 0.5443 - mean_absolute_percentage_error: 9.7089 - mean_squared_error: 0.4931 - val_loss: 0.5166 - val_mean_absolute_error: 0.5647 - val_mean_absolute_percentage_error: 10.0715 - val_mean_squared_error: 0.5166\n",
      "Epoch 28/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4992 - mean_absolute_error: 0.5583 - mean_absolute_percentage_error: 9.8406 - mean_squared_error: 0.4992 - val_loss: 0.5201 - val_mean_absolute_error: 0.5689 - val_mean_absolute_percentage_error: 9.8061 - val_mean_squared_error: 0.5201\n",
      "Epoch 29/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5117 - mean_absolute_error: 0.5569 - mean_absolute_percentage_error: 9.8131 - mean_squared_error: 0.5117 - val_loss: 0.5030 - val_mean_absolute_error: 0.5583 - val_mean_absolute_percentage_error: 9.8151 - val_mean_squared_error: 0.5030\n",
      "Epoch 30/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.5196 - mean_absolute_error: 0.5602 - mean_absolute_percentage_error: 9.9759 - mean_squared_error: 0.5196 - val_loss: 0.5015 - val_mean_absolute_error: 0.5588 - val_mean_absolute_percentage_error: 9.8427 - val_mean_squared_error: 0.5015\n",
      "Epoch 31/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5126 - mean_absolute_error: 0.5616 - mean_absolute_percentage_error: 9.8001 - mean_squared_error: 0.5126 - val_loss: 0.4975 - val_mean_absolute_error: 0.5554 - val_mean_absolute_percentage_error: 9.7683 - val_mean_squared_error: 0.4975\n",
      "Epoch 32/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4660 - mean_absolute_error: 0.5365 - mean_absolute_percentage_error: 9.4520 - mean_squared_error: 0.4660 - val_loss: 0.4973 - val_mean_absolute_error: 0.5551 - val_mean_absolute_percentage_error: 9.8002 - val_mean_squared_error: 0.4973\n",
      "Epoch 33/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4878 - mean_absolute_error: 0.5494 - mean_absolute_percentage_error: 9.6488 - mean_squared_error: 0.4878 - val_loss: 0.5030 - val_mean_absolute_error: 0.5595 - val_mean_absolute_percentage_error: 9.7446 - val_mean_squared_error: 0.5030\n",
      "Epoch 34/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4859 - mean_absolute_error: 0.5433 - mean_absolute_percentage_error: 9.5395 - mean_squared_error: 0.4859 - val_loss: 0.4950 - val_mean_absolute_error: 0.5561 - val_mean_absolute_percentage_error: 9.7546 - val_mean_squared_error: 0.4950\n",
      "Epoch 35/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4739 - mean_absolute_error: 0.5360 - mean_absolute_percentage_error: 9.3809 - mean_squared_error: 0.4739 - val_loss: 0.4965 - val_mean_absolute_error: 0.5573 - val_mean_absolute_percentage_error: 9.7425 - val_mean_squared_error: 0.4965\n",
      "Epoch 36/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5037 - mean_absolute_error: 0.5519 - mean_absolute_percentage_error: 9.7226 - mean_squared_error: 0.5037 - val_loss: 0.4978 - val_mean_absolute_error: 0.5577 - val_mean_absolute_percentage_error: 9.8628 - val_mean_squared_error: 0.4978\n",
      "Epoch 37/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4866 - mean_absolute_error: 0.5474 - mean_absolute_percentage_error: 9.6238 - mean_squared_error: 0.4866 - val_loss: 0.4909 - val_mean_absolute_error: 0.5517 - val_mean_absolute_percentage_error: 9.6335 - val_mean_squared_error: 0.4909\n",
      "Epoch 38/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4873 - mean_absolute_error: 0.5480 - mean_absolute_percentage_error: 9.6813 - mean_squared_error: 0.4873 - val_loss: 0.5009 - val_mean_absolute_error: 0.5596 - val_mean_absolute_percentage_error: 9.8631 - val_mean_squared_error: 0.5009\n",
      "Epoch 39/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4693 - mean_absolute_error: 0.5434 - mean_absolute_percentage_error: 9.5164 - mean_squared_error: 0.4693 - val_loss: 0.4953 - val_mean_absolute_error: 0.5552 - val_mean_absolute_percentage_error: 9.6351 - val_mean_squared_error: 0.4953\n",
      "Epoch 40/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4560 - mean_absolute_error: 0.5293 - mean_absolute_percentage_error: 9.2885 - mean_squared_error: 0.4560 - val_loss: 0.4907 - val_mean_absolute_error: 0.5551 - val_mean_absolute_percentage_error: 9.7766 - val_mean_squared_error: 0.4907\n",
      "Epoch 41/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4793 - mean_absolute_error: 0.5404 - mean_absolute_percentage_error: 9.4704 - mean_squared_error: 0.4793 - val_loss: 0.4909 - val_mean_absolute_error: 0.5531 - val_mean_absolute_percentage_error: 9.5953 - val_mean_squared_error: 0.4909\n",
      "Epoch 42/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.4732 - mean_absolute_error: 0.5359 - mean_absolute_percentage_error: 9.4113 - mean_squared_error: 0.4732 - val_loss: 0.4994 - val_mean_absolute_error: 0.5572 - val_mean_absolute_percentage_error: 9.7474 - val_mean_squared_error: 0.4994\n",
      "Epoch 43/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4850 - mean_absolute_error: 0.5436 - mean_absolute_percentage_error: 9.5655 - mean_squared_error: 0.4850 - val_loss: 0.5256 - val_mean_absolute_error: 0.5754 - val_mean_absolute_percentage_error: 10.3513 - val_mean_squared_error: 0.5256\n",
      "Epoch 44/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4853 - mean_absolute_error: 0.5523 - mean_absolute_percentage_error: 9.7742 - mean_squared_error: 0.4853 - val_loss: 0.4865 - val_mean_absolute_error: 0.5520 - val_mean_absolute_percentage_error: 9.6897 - val_mean_squared_error: 0.4865\n",
      "Epoch 45/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4965 - mean_absolute_error: 0.5490 - mean_absolute_percentage_error: 9.6933 - mean_squared_error: 0.4965 - val_loss: 0.4956 - val_mean_absolute_error: 0.5541 - val_mean_absolute_percentage_error: 9.4954 - val_mean_squared_error: 0.4956\n",
      "Epoch 46/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4896 - mean_absolute_error: 0.5456 - mean_absolute_percentage_error: 9.5775 - mean_squared_error: 0.4896 - val_loss: 0.4931 - val_mean_absolute_error: 0.5570 - val_mean_absolute_percentage_error: 9.8789 - val_mean_squared_error: 0.4931\n",
      "Epoch 47/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4859 - mean_absolute_error: 0.5457 - mean_absolute_percentage_error: 9.5853 - mean_squared_error: 0.4859 - val_loss: 0.5062 - val_mean_absolute_error: 0.5654 - val_mean_absolute_percentage_error: 10.1016 - val_mean_squared_error: 0.5062\n",
      "Epoch 48/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4752 - mean_absolute_error: 0.5378 - mean_absolute_percentage_error: 9.5845 - mean_squared_error: 0.4752 - val_loss: 0.4880 - val_mean_absolute_error: 0.5541 - val_mean_absolute_percentage_error: 9.7153 - val_mean_squared_error: 0.4880\n",
      "Epoch 49/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4562 - mean_absolute_error: 0.5279 - mean_absolute_percentage_error: 9.2433 - mean_squared_error: 0.4562 - val_loss: 0.4915 - val_mean_absolute_error: 0.5539 - val_mean_absolute_percentage_error: 9.6843 - val_mean_squared_error: 0.4915\n",
      "Epoch 50/50\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - loss: 0.4765 - mean_absolute_error: 0.5457 - mean_absolute_percentage_error: 9.6427 - mean_squared_error: 0.4765 - val_loss: 0.4843 - val_mean_absolute_error: 0.5497 - val_mean_absolute_percentage_error: 9.5093 - val_mean_squared_error: 0.4843\n",
      "{'loss': [20.262296676635742, 4.198235988616943, 2.4780445098876953, 1.9253910779953003, 1.597399353981018, 1.3647655248641968, 1.1812496185302734, 1.0314992666244507, 0.9266972541809082, 0.8334267139434814, 0.7718547582626343, 0.7153836488723755, 0.674586832523346, 0.6476117372512817, 0.6163479089736938, 0.5974730849266052, 0.5843517780303955, 0.5683237910270691, 0.5619162917137146, 0.5510439872741699, 0.5475594997406006, 0.5392339825630188, 0.5283923745155334, 0.5250424742698669, 0.5200068354606628, 0.5264356136322021, 0.5072050094604492, 0.5100005269050598, 0.5055573582649231, 0.49833178520202637, 0.49490275979042053, 0.4933679699897766, 0.4933571517467499, 0.49090489745140076, 0.48861730098724365, 0.4874870181083679, 0.48836758732795715, 0.48334044218063354, 0.479920357465744, 0.47589871287345886, 0.4779232144355774, 0.4724574685096741, 0.4736396372318268, 0.47583821415901184, 0.46956315636634827, 0.47104668617248535, 0.47059351205825806, 0.47881725430488586, 0.4672566056251526, 0.46718472242355347], 'mean_absolute_error': [4.181402206420898, 1.5807509422302246, 1.1978542804718018, 1.0575103759765625, 0.9658035039901733, 0.8883505463600159, 0.8284652829170227, 0.7738779187202454, 0.7358182668685913, 0.6996062994003296, 0.6771201491355896, 0.6529695391654968, 0.6341320276260376, 0.6228561401367188, 0.6084267497062683, 0.5982205271720886, 0.5940073728561401, 0.5867526531219482, 0.5834221839904785, 0.5781704783439636, 0.5761331915855408, 0.5733960866928101, 0.5668003559112549, 0.5659353733062744, 0.5627050995826721, 0.5685878992080688, 0.5580673217773438, 0.5595252513885498, 0.5541231036186218, 0.5518078207969666, 0.551588237285614, 0.5498853325843811, 0.5500966310501099, 0.5476806163787842, 0.5451842546463013, 0.5477470755577087, 0.5487610697746277, 0.5439102649688721, 0.5427654385566711, 0.5404289364814758, 0.5424767136573792, 0.5388243198394775, 0.5389302372932434, 0.542815089225769, 0.5359971523284912, 0.5381780862808228, 0.5387938022613525, 0.5402803421020508, 0.534371018409729, 0.5363480448722839], 'mean_absolute_percentage_error': [70.62552642822266, 27.417179107666016, 20.91985321044922, 18.566930770874023, 16.9625186920166, 15.628451347351074, 14.585198402404785, 13.64853572845459, 12.953909873962402, 12.362214088439941, 11.93023681640625, 11.542587280273438, 11.190253257751465, 11.022048950195312, 10.770244598388672, 10.600137710571289, 10.509031295776367, 10.380319595336914, 10.356369018554688, 10.235395431518555, 10.193825721740723, 10.146081924438477, 10.048134803771973, 10.027167320251465, 9.964849472045898, 10.05616569519043, 9.866122245788574, 9.911189079284668, 9.81302261352539, 9.76186466217041, 9.75865364074707, 9.708455085754395, 9.732190132141113, 9.679156303405762, 9.63364315032959, 9.683314323425293, 9.706798553466797, 9.612027168273926, 9.596759796142578, 9.55325984954834, 9.582513809204102, 9.505708694458008, 9.513495445251465, 9.576966285705566, 9.482237815856934, 9.482833862304688, 9.498838424682617, 9.558972358703613, 9.416497230529785, 9.466660499572754], 'mean_squared_error': [20.262296676635742, 4.198235988616943, 2.4780445098876953, 1.9253910779953003, 1.597399353981018, 1.3647655248641968, 1.1812496185302734, 1.0314992666244507, 0.9266972541809082, 0.8334267139434814, 0.7718547582626343, 0.7153836488723755, 0.674586832523346, 0.6476117372512817, 0.6163479089736938, 0.5974730849266052, 0.5843517780303955, 0.5683237910270691, 0.5619162917137146, 0.5510439872741699, 0.5475594997406006, 0.5392339825630188, 0.5283923745155334, 0.5250424742698669, 0.5200068354606628, 0.5264356136322021, 0.5072050094604492, 0.5100005269050598, 0.5055573582649231, 0.49833178520202637, 0.49490275979042053, 0.4933679699897766, 0.4933571517467499, 0.49090489745140076, 0.48861730098724365, 0.4874870181083679, 0.48836758732795715, 0.48334044218063354, 0.479920357465744, 0.47589871287345886, 0.4779232144355774, 0.4724574685096741, 0.4736396372318268, 0.47583821415901184, 0.46956315636634827, 0.47104668617248535, 0.47059351205825806, 0.47881725430488586, 0.4672566056251526, 0.46718472242355347], 'val_loss': [6.827681541442871, 3.1266915798187256, 2.178812265396118, 1.7322535514831543, 1.467186450958252, 1.2491201162338257, 1.084923267364502, 0.9645742177963257, 0.8591774106025696, 0.7915509343147278, 0.7350877523422241, 0.683815062046051, 0.6604128479957581, 0.6307547688484192, 0.5964033603668213, 0.5772235989570618, 0.5652662515640259, 0.5836922526359558, 0.5454347133636475, 0.5395179986953735, 0.5411467552185059, 0.540602445602417, 0.5340733528137207, 0.5142830610275269, 0.5106932520866394, 0.5075717568397522, 0.5166114568710327, 0.5201404094696045, 0.503041684627533, 0.5014542937278748, 0.4975026845932007, 0.497268944978714, 0.5030030608177185, 0.49501582980155945, 0.49647191166877747, 0.49781525135040283, 0.4908923804759979, 0.5008551478385925, 0.49533388018608093, 0.49069124460220337, 0.490859717130661, 0.49941641092300415, 0.5255973935127258, 0.4864663779735565, 0.4955746829509735, 0.4931245446205139, 0.5061554312705994, 0.48798972368240356, 0.4915039539337158, 0.48433709144592285], 'val_mean_absolute_error': [2.086827039718628, 1.3531594276428223, 1.1386829614639282, 1.0206507444381714, 0.9387037754058838, 0.8670699000358582, 0.8075271844863892, 0.7610006332397461, 0.7169921398162842, 0.6912311315536499, 0.6668487787246704, 0.6438813209533691, 0.6341408491134644, 0.6192718744277954, 0.6022177338600159, 0.59244304895401, 0.5860175490379333, 0.5961952805519104, 0.576173722743988, 0.5708959102630615, 0.5730406641960144, 0.5751073956489563, 0.5719434022903442, 0.5605448484420776, 0.5594154000282288, 0.5579809546470642, 0.5646763443946838, 0.5689442157745361, 0.5583086013793945, 0.5588475465774536, 0.5554417371749878, 0.5551115870475769, 0.5595318675041199, 0.5560718178749084, 0.5573101043701172, 0.5576556324958801, 0.5517449975013733, 0.5595518350601196, 0.5551788806915283, 0.5551127195358276, 0.553062915802002, 0.5571797490119934, 0.5753667950630188, 0.5519908666610718, 0.5541211366653442, 0.5569524765014648, 0.5653877854347229, 0.5541359186172485, 0.5538946986198425, 0.5497328042984009], 'val_mean_absolute_percentage_error': [35.708412170410156, 23.45818519592285, 19.734010696411133, 17.722902297973633, 16.382808685302734, 15.029741287231445, 14.106964111328125, 13.235641479492188, 12.492968559265137, 11.923017501831055, 11.7346830368042, 11.273212432861328, 11.204150199890137, 10.918496131896973, 10.58442211151123, 10.349810600280762, 10.310997009277344, 10.656225204467773, 10.105448722839355, 9.97616958618164, 10.18209171295166, 10.282942771911621, 10.202122688293457, 9.767807960510254, 9.708364486694336, 9.647017478942871, 10.071481704711914, 9.806059837341309, 9.81505012512207, 9.842716217041016, 9.768296241760254, 9.800176620483398, 9.744600296020508, 9.7545747756958, 9.742520332336426, 9.862837791442871, 9.633481979370117, 9.863125801086426, 9.635113716125488, 9.776576042175293, 9.595344543457031, 9.74739933013916, 10.351296424865723, 9.689693450927734, 9.495441436767578, 9.878889083862305, 10.101582527160645, 9.715314865112305, 9.68428897857666, 9.50934886932373], 'val_mean_squared_error': [6.827681541442871, 3.1266915798187256, 2.178812265396118, 1.7322535514831543, 1.467186450958252, 1.2491201162338257, 1.084923267364502, 0.9645742177963257, 0.8591774106025696, 0.7915509343147278, 0.7350877523422241, 0.683815062046051, 0.6604128479957581, 0.6307547688484192, 0.5964033603668213, 0.5772235989570618, 0.5652662515640259, 0.5836922526359558, 0.5454347133636475, 0.5395179986953735, 0.5411467552185059, 0.540602445602417, 0.5340733528137207, 0.5142830610275269, 0.5106932520866394, 0.5075717568397522, 0.5166114568710327, 0.5201404094696045, 0.503041684627533, 0.5014542937278748, 0.4975026845932007, 0.497268944978714, 0.5030030608177185, 0.49501582980155945, 0.49647191166877747, 0.49781525135040283, 0.4908923804759979, 0.5008551478385925, 0.49533388018608093, 0.49069124460220337, 0.490859717130661, 0.49941641092300415, 0.5255973935127258, 0.4864663779735565, 0.4955746829509735, 0.4931245446205139, 0.5061554312705994, 0.48798972368240356, 0.4915039539337158, 0.48433709144592285]}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "fab0a3ba9ab68eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:52:32.857616Z",
     "start_time": "2025-04-05T20:52:32.325456Z"
    }
   },
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss = model.evaluate(features_test, target_test, verbose=1)\n",
    "\n",
    "# Print the test loss\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(features_test)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.5316 - mean_absolute_error: 0.5643 - mean_absolute_percentage_error: 9.6062 - mean_squared_error: 0.5316\n",
      "Test loss: [0.5440133213996887, 0.5440133213996887, 0.5727120041847229, 9.751933097839355]\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "9bd41725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:56:37.119267Z",
     "start_time": "2025-04-05T20:55:10.297988Z"
    }
   },
   "source": [
    "\n",
    "# Initialize lists to store metrics\n",
    "train_r_values = []\n",
    "train_mse_values = []\n",
    "train_mape_values = []\n",
    "test_r_values = []\n",
    "test_mse_values = []\n",
    "test_mape_values = []\n",
    "\n",
    "# Function to calculate metrics (R, MSE, MAPE)\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Calculate R (Pearson correlation coefficient)   \n",
    "    r, _ = pearsonr(y_true, y_pred)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    return r, mse, mape\n",
    "\n",
    "# Repeat the experiment for random_state 1 to 100\n",
    "for random_state in range(1, 10):\n",
    "    # Split the data into training and temporary sets\n",
    "    features_train, features_temp, target_train, target_temp = train_test_split(features_scaled, target, test_size=0.4, random_state=random_state)\n",
    "    \n",
    "    # Split the temporary set into testing and validation sets\n",
    "    features_test, features_validate, target_test, target_validate = train_test_split(features_temp, target_temp, test_size=0.5, random_state=random_state)\n",
    "    \n",
    "    # Initialize the Sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the input layer with 11 features\n",
    "    model.add(Input(shape=(11,)))  # Input layer with 11 features\n",
    "    \n",
    "    # Add the first hidden layer with 15 neurons and ReLU activation\n",
    "    model.add(Dense(15, activation='relu'))  # First hidden layer\n",
    "    \n",
    "    # Add the second hidden layer with 15 neurons and ReLU activation\n",
    "    model.add(Dense(15, activation='relu'))  # Second hidden layer\n",
    "    \n",
    "    # Add the third hidden layer with 15 neurons and ReLU activation\n",
    "    model.add(Dense(15, activation='relu'))  # Third hidden layer\n",
    "    \n",
    "    # Add the output layer with 1 neuron and linear activation\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    # Compile the model with mean squared error loss and Adam optimizer\n",
    "    # Also include mean squared error, mean absolute error, and mean absolute percentage error as metrics\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "    \n",
    "    # Train the model with the training data\n",
    "    history = model.fit(features_train, target_train, epochs=50, batch_size=32, validation_data=(features_validate, target_validate), verbose=0)\n",
    "    \n",
    "    # Make predictions on the training and test data\n",
    "    train_predictions = model.predict(features_train)\n",
    "    test_predictions = model.predict(features_test)\n",
    "    \n",
    "    # Calculate metrics for training data set\n",
    "    train_r, train_mse, train_mape = calculate_metrics(target_train, train_predictions)\n",
    "    \n",
    "    # Calculate metrics for testing data set\n",
    "    test_r, test_mse, test_mape = calculate_metrics(target_test, test_predictions)\n",
    "    \n",
    "    # Store the metrics in the lists\n",
    "    train_r_values.append(train_r)\n",
    "    train_mse_values.append(train_mse)\n",
    "    train_mape_values.append(train_mape)\n",
    "    test_r_values.append(test_r)\n",
    "    test_mse_values.append(test_mse)\n",
    "    test_mape_values.append(test_mape)\n",
    "\n",
    "    print(random_state)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 927us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 760us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 758us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 922us/step\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 832us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 884us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 840us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 901us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step  \n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 974us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n",
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\u001B[1m92/92\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1ms/step\n",
      "\u001B[1m31/31\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 943us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43013/1950007655.py:12: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  r, _ = pearsonr(y_true, y_pred)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c29d3303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:57:44.449956Z",
     "start_time": "2025-04-05T20:57:44.445725Z"
    }
   },
   "source": [
    "# Reference values for ANN model\n",
    "reference_values_ann = {\n",
    "    \"Training Data Set\": {\"R\": 0.66, \"MSE\": 0.37, \"MAPE\": 0.14},\n",
    "    \"Testing Data Set\": {\"R\": 0.58, \"MSE\": 0.4, \"MAPE\": 0.12}\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "ca88ec9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T20:57:49.182582Z",
     "start_time": "2025-04-05T20:57:48.035137Z"
    }
   },
   "source": [
    "\n",
    "# Print collected metrics for verification\n",
    "print(\"Training Data Set Metrics:\")\n",
    "print(\"R values:\", train_r_values)\n",
    "print(\"MSE values:\", train_mse_values)\n",
    "print(\"MAPE values:\", train_mape_values)\n",
    "\n",
    "print(\"\\nTesting Data Set Metrics:\")\n",
    "print(\"R values:\", test_r_values)\n",
    "print(\"MSE values:\", test_mse_values)\n",
    "print(\"MAPE values:\", test_mape_values)\n",
    "\n",
    "# Visualize the metrics in Gaussian curves along with reference values from ANN model\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.hist(train_r_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Training Data Set\"][\"R\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Training Data Set R Values')\n",
    "plt.xlabel('R')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.hist(test_r_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Testing Data Set\"][\"R\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Testing Data Set R Values')\n",
    "plt.xlabel('R')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.hist(train_mse_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Training Data Set\"][\"MSE\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Training Data Set MSE Values')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.hist(test_mse_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Testing Data Set\"][\"MSE\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Testing Data Set MSE Values')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.hist(train_mape_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Training Data Set\"][\"MAPE\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Training Data Set MAPE Values')\n",
    "plt.xlabel('MAPE')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.hist(test_mape_values, bins=30, density=True)\n",
    "plt.axvline(reference_values_ann[\"Testing Data Set\"][\"MAPE\"], color='r', linestyle='dashed', linewidth=2)\n",
    "plt.title('Testing Data Set MAPE Values')\n",
    "plt.xlabel('MAPE')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Set Metrics:\n",
      "R values: [array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan]), array([nan, nan, nan, ..., nan, nan, nan])]\n",
      "MSE values: [0.4570007920265198, 0.4386994242668152, 0.43422073125839233, 0.44980499148368835, 0.4804772138595581, 0.44346383213996887, 0.454471230506897, 0.4513114392757416, 0.45432451367378235]\n",
      "MAPE values: [0.09487459808588028, 0.09074681252241135, 0.08824052661657333, 0.0909239649772644, 0.09341134876012802, 0.09121543914079666, 0.09458625316619873, 0.09132010489702225, 0.09205880761146545]\n",
      "\n",
      "Testing Data Set Metrics:\n",
      "R values: [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan]), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan])]\n",
      "MSE values: [0.5276978611946106, 0.4995175004005432, 0.5558081865310669, 0.551017701625824, 0.4908028244972229, 0.48665347695350647, 0.5507200360298157, 0.4856337010860443, 0.5360199809074402]\n",
      "MAPE values: [0.09821601957082748, 0.09565957635641098, 0.09903910011053085, 0.10389673709869385, 0.09746158868074417, 0.09677731990814209, 0.10287489742040634, 0.09468469768762589, 0.10123054683208466]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abroer/miniconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:7065: RuntimeWarning: All-NaN slice encountered\n",
      "  xmin = min(xmin, np.nanmin(xi))\n",
      "/home/abroer/miniconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:7066: RuntimeWarning: All-NaN slice encountered\n",
      "  xmax = max(xmax, np.nanmax(xi))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m8\u001B[39m))\n\u001B[1;32m     15\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_r_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdensity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m plt\u001B[38;5;241m.\u001B[39maxvline(reference_values_ann[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Data Set\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR\u001B[39m\u001B[38;5;124m\"\u001B[39m], color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m, linestyle\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdashed\u001B[39m\u001B[38;5;124m'\u001B[39m, linewidth\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     18\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining Data Set R Values\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:453\u001B[0m, in \u001B[0;36mmake_keyword_only.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m name_idx:\n\u001B[1;32m    448\u001B[0m     warn_deprecated(\n\u001B[1;32m    449\u001B[0m         since, message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing the \u001B[39m\u001B[38;5;132;01m%(name)s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%(obj_type)s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositionally is deprecated since Matplotlib \u001B[39m\u001B[38;5;132;01m%(since)s\u001B[39;00m\u001B[38;5;124m; the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter will become keyword-only in \u001B[39m\u001B[38;5;132;01m%(removal)s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    452\u001B[0m         name\u001B[38;5;241m=\u001B[39mname, obj_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/pyplot.py:3469\u001B[0m, in \u001B[0;36mhist\u001B[0;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001B[0m\n\u001B[1;32m   3444\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mhist)\n\u001B[1;32m   3445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhist\u001B[39m(\n\u001B[1;32m   3446\u001B[0m     x: ArrayLike \u001B[38;5;241m|\u001B[39m Sequence[ArrayLike],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3467\u001B[0m     BarContainer \u001B[38;5;241m|\u001B[39m Polygon \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[BarContainer \u001B[38;5;241m|\u001B[39m Polygon],\n\u001B[1;32m   3468\u001B[0m ]:\n\u001B[0;32m-> 3469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhist\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3471\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbins\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3472\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3473\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdensity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdensity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3474\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcumulative\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcumulative\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbottom\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbottom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhisttype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhisttype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3478\u001B[0m \u001B[43m        \u001B[49m\u001B[43malign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malign\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3479\u001B[0m \u001B[43m        \u001B[49m\u001B[43morientation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morientation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrwidth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrwidth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstacked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstacked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3485\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3486\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3487\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/_api/deprecation.py:453\u001B[0m, in \u001B[0;36mmake_keyword_only.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m name_idx:\n\u001B[1;32m    448\u001B[0m     warn_deprecated(\n\u001B[1;32m    449\u001B[0m         since, message\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing the \u001B[39m\u001B[38;5;132;01m%(name)s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%(obj_type)s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositionally is deprecated since Matplotlib \u001B[39m\u001B[38;5;132;01m%(since)s\u001B[39;00m\u001B[38;5;124m; the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter will become keyword-only in \u001B[39m\u001B[38;5;132;01m%(removal)s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    452\u001B[0m         name\u001B[38;5;241m=\u001B[39mname, obj_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/__init__.py:1521\u001B[0m, in \u001B[0;36m_preprocess_data.<locals>.inner\u001B[0;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m   1519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(ax, \u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1520\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1521\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[43m            \u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1523\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcbook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msanitize_sequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mcbook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msanitize_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1526\u001B[0m     bound \u001B[38;5;241m=\u001B[39m new_sig\u001B[38;5;241m.\u001B[39mbind(ax, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1527\u001B[0m     auto_label \u001B[38;5;241m=\u001B[39m (bound\u001B[38;5;241m.\u001B[39marguments\u001B[38;5;241m.\u001B[39mget(label_namer)\n\u001B[1;32m   1528\u001B[0m                   \u001B[38;5;129;01mor\u001B[39;00m bound\u001B[38;5;241m.\u001B[39mkwargs\u001B[38;5;241m.\u001B[39mget(label_namer))\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:7078\u001B[0m, in \u001B[0;36mAxes.hist\u001B[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001B[0m\n\u001B[1;32m   7076\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   7077\u001B[0m         _w \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 7078\u001B[0m     bins \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistogram_bin_edges\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   7079\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbin_range\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_w\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   7080\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   7081\u001B[0m     hist_kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrange\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m bin_range\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/lib/_histograms_impl.py:677\u001B[0m, in \u001B[0;36mhistogram_bin_edges\u001B[0;34m(a, bins, range, weights)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;124;03mFunction to calculate only the edges of the bins used by the `histogram`\u001B[39;00m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;124;03mfunction.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    674\u001B[0m \n\u001B[1;32m    675\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    676\u001B[0m a, weights \u001B[38;5;241m=\u001B[39m _ravel_and_check_weights(a, weights)\n\u001B[0;32m--> 677\u001B[0m bin_edges, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_get_bin_edges\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bin_edges\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/lib/_histograms_impl.py:430\u001B[0m, in \u001B[0;36m_get_bin_edges\u001B[0;34m(a, bins, range, weights)\u001B[0m\n\u001B[1;32m    427\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_equal_bins \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    428\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`bins` must be positive, when an integer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 430\u001B[0m     first_edge, last_edge \u001B[38;5;241m=\u001B[39m \u001B[43m_get_outer_edges\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mndim(bins) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    433\u001B[0m     bin_edges \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(bins)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/numpy/lib/_histograms_impl.py:323\u001B[0m, in \u001B[0;36m_get_outer_edges\u001B[0;34m(a, range)\u001B[0m\n\u001B[1;32m    321\u001B[0m     first_edge, last_edge \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mmin(), a\u001B[38;5;241m.\u001B[39mmax()\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (np\u001B[38;5;241m.\u001B[39misfinite(first_edge) \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39misfinite(last_edge)):\n\u001B[0;32m--> 323\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    324\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautodetected range of [\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m] is not finite\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(first_edge, last_edge))\n\u001B[1;32m    326\u001B[0m \u001B[38;5;66;03m# expand empty range to avoid divide by zero\u001B[39;00m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_edge \u001B[38;5;241m==\u001B[39m last_edge:\n",
      "\u001B[0;31mValueError\u001B[0m: autodetected range of [nan, nan] is not finite"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
